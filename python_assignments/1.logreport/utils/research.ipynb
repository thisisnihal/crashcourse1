{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07174d91",
   "metadata": {},
   "source": [
    "### Problem \n",
    "1.You are given a raw log file which contains messages in JSON format.\n",
    "\n",
    "Example: `{“timestamp”: “2025-10-13 20:05:05”, “level”: “ERROR”, “message”: “DB failed to connect”, “service”: “UserService”}`\n",
    "\n",
    "The log file could be very large also in GBs.  \n",
    "Please create a report which contains with the following:  \n",
    "•\tCount of errors per service  :done\n",
    "•\tMost common error messages  \n",
    "•\tCount of each log levels  :done\n",
    "•\tMost 5 occurred logs with their count  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "https://stackoverflow.com/questions/10382253/reading-rather-large-json-files\n",
    "\n",
    "http://pypi.python.org/pypi/ijson/\n",
    "\n",
    "https://pythonspeed.com/articles/json-memory-streaming/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1de536d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log per level: {'WARNING': 1676593, 'INFO': 1681122, 'CRITICAL': 1678492, 'ERROR': 1679424, 'DEBUG': 1677626}\n",
      "\n",
      "error per service: {'InventoryService': 335647, 'AuthService': 336236, 'OrderService': 335761, 'AnalyticsService': 336052, 'PaymentService': 335728}\n",
      "\n",
      "most occured errors:  [('User login successful', 279674), ('Payment gateway timeout', 280188), ('Order created successfully', 280074), ('Database connection lost', 279772), ('Cache miss occurred', 280735)]\n",
      "\n",
      "most occured logs:  [(('WARNING', 'AnalyticsService', 'Database connection lost'), 56449), (('INFO', 'InventoryService', 'User login successful'), 56364), (('INFO', 'AnalyticsService', 'Cache miss occurred'), 56408), (('CRITICAL', 'InventoryService', 'User login successful'), 56497), (('CRITICAL', 'AuthService', 'External API request failed'), 56416)]\n",
      "\n",
      "Execution time: 16.7083 seconds\n"
     ]
    }
   ],
   "source": [
    "# using ijson\n",
    "\n",
    "import ijson\n",
    "import heapq\n",
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "log_level_freq = {}     # {Key.ERROR: 56, 'WARNING': 12}\n",
    "error_service_freq = {} # {'UserService': 12, 'AuthService': 12}\n",
    "top_logs = []           # a heap stores top k logs\n",
    "top_errors = []         # a heap stores top k error\n",
    "log_counter = {}        # {'log_message_x': 2}\n",
    "error_counter = {}      # {'error_message_1': 4}\n",
    "\n",
    "TOP_K = 5\n",
    "FILE_PATH = 'data/logs_1000mb.json'\n",
    "class Key:\n",
    "    LEVEL = 'level'\n",
    "    ERROR = 'ERROR'\n",
    "    SERVICE = 'service'\n",
    "    MESSAGE = 'message'\n",
    "\n",
    "with open(FILE_PATH, 'r') as f:\n",
    "    for log in ijson.items(f, 'item'):\n",
    "        # print(record)\n",
    "        if log[Key.LEVEL]:\n",
    "            log_level_freq[log[Key.LEVEL]] = log_level_freq.get(log[Key.LEVEL], 0) + 1\n",
    "        if log[Key.LEVEL] == Key.ERROR and log[Key.SERVICE]:\n",
    "            error_service_freq[log[Key.SERVICE]] = error_service_freq.get(log[Key.SERVICE], 0) + 1\n",
    "\n",
    "        log_key = (log.get(Key.LEVEL,''), log.get(Key.SERVICE,''), log.get(Key.MESSAGE,''))\n",
    "        log_count = log_counter.get(log_key, 0) + 1\n",
    "\n",
    "        log_counter[log_key] = log_count\n",
    "        if log.get(Key.LEVEL) == Key.ERROR:\n",
    "            msg = log.get(Key.MESSAGE, '')\n",
    "            error_counter[msg] = error_counter.get(msg, 0) + 1\n",
    "            \n",
    "top_logs = heapq.nlargest(TOP_K, log_counter.items(), key=lambda x: x[1])\n",
    "top_errors = heapq.nlargest(TOP_K, error_counter.items(), key=lambda x: x[1])\n",
    "\n",
    "\n",
    "\n",
    "print('log per level:', log_level_freq)\n",
    "print('\\nerror per service:', error_service_freq)\n",
    "\n",
    "most_occured_logs = sorted(top_logs, reverse=True)\n",
    "most_occured_errors = sorted(top_errors, reverse=True)\n",
    "\n",
    "print(\"\\nmost occured errors: \", most_occured_errors)\n",
    "print(\"\\nmost occured logs: \", most_occured_logs)\n",
    "\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"\\nExecution time: {execution_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6d6e31",
   "metadata": {},
   "source": [
    "\n",
    "log per level: {'WARNING': 1676593, 'INFO': 1681122, 'CRITICAL': 1678492, 'ERROR': 1679424, 'DEBUG': 1677626}\n",
    "\n",
    "error per service: {'InventoryService': 335647, 'AuthService': 336236, 'OrderService': 335761, 'AnalyticsService': 336052, 'PaymentService': 335728} most occured errors: [(280735, 'Cache miss occurred'), (280734, 'Cache miss occurred'), (280733, 'Cache miss occurred'), (280732, 'Cache miss occurred'), (280731, 'Cache miss occurred')]\n",
    "\n",
    "most occured logs: [(56497, ('CRITICAL', 'InventoryService', 'User login successful')), (56496, ('CRITICAL', 'InventoryService', 'User login successful')), (56495, ('CRITICAL', 'InventoryService', 'User login successful')), (56494, ('CRITICAL', 'InventoryService', 'User login successful')), (56493, ('CRITICAL', 'InventoryService', 'User login successful'))]\n",
    "\n",
    "Execution time: 26.9048 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b43ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "class KthLargest:\n",
    "    def __init__(self, k: int, data):\n",
    "        \"\"\"\"\"\"\n",
    "        self.minHeap = []\n",
    "        self.k = k\n",
    "        for e in data:\n",
    "            heapq.heappush(self.minHeap, e)\n",
    "            if len(self.minHeap) > k:\n",
    "                heapq.heappop(self.minHeap)\n",
    "\n",
    "    def add(self, val: int) -> int:\n",
    "        heapq.heappush(self.minHeap, val)\n",
    "        if len(self.minHeap) > self.k:\n",
    "            heapq.heappop(self.minHeap)\n",
    "        return self.minHeap[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054a7823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log per level: {'WARNING': 1676593, 'INFO': 1681122, 'CRITICAL': 1678492, 'ERROR': 1679424, 'DEBUG': 1677626}\n",
      "\n",
      "error per service: {'InventoryService': 335647, 'AuthService': 336236, 'OrderService': 335761, 'AnalyticsService': 336052, 'PaymentService': 335728}\n",
      "\n",
      "most occured errors:  [(280735, 'Cache miss occurred'), (280734, 'Cache miss occurred'), (280733, 'Cache miss occurred'), (280732, 'Cache miss occurred'), (280731, 'Cache miss occurred')]\n",
      "\n",
      "most occured logs:  [(56497, ('CRITICAL', 'InventoryService', 'User login successful')), (56496, ('CRITICAL', 'InventoryService', 'User login successful')), (56495, ('CRITICAL', 'InventoryService', 'User login successful')), (56494, ('CRITICAL', 'InventoryService', 'User login successful')), (56493, ('CRITICAL', 'InventoryService', 'User login successful'))]\n",
      "\n",
      "Execution time: 26.9048 seconds\n"
     ]
    }
   ],
   "source": [
    "# using json\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import heapq\n",
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "log_level_freq = {}     # {Key.ERROR: 56, 'WARNING': 12}\n",
    "error_service_freq = {} # {'UserService': 12, 'AuthService': 12}\n",
    "top_logs = []           # a heap stores top k logs\n",
    "top_errors = []         # a heap stores top k error\n",
    "log_counter = {}        # {'log_message_x': 2}\n",
    "error_counter = {}      # {'error_message_1': 4}\n",
    "FILE_PATH = 'data/logs_1000mb.json'\n",
    "TOP_K = 5\n",
    "class Key:\n",
    "    LEVEL = 'level'\n",
    "    ERROR = 'ERROR'\n",
    "    SERVICE = 'service'\n",
    "    MESSAGE = 'message'\n",
    "\n",
    "f = open(FILE_PATH)\n",
    "import json\n",
    "logs = json.load(f)\n",
    "for index, log in enumerate(logs):\n",
    "        if log[Key.LEVEL]:\n",
    "            log_level_freq[log[Key.LEVEL]] = log_level_freq.get(log[Key.LEVEL], 0) + 1\n",
    "        if log[Key.LEVEL] == Key.ERROR and log[Key.SERVICE]:\n",
    "            error_service_freq[log[Key.SERVICE]] = error_service_freq.get(log[Key.SERVICE], 0) + 1\n",
    "\n",
    "        log_key = (log.get(Key.LEVEL,''), log.get(Key.SERVICE,''), log.get(Key.MESSAGE,''))\n",
    "        log_count = log_counter.get(log_key, 0) + 1\n",
    "\n",
    "        log_counter[log_key] = log_count\n",
    "        heapq.heappush(top_logs, (log_count, log_key))\n",
    "        if len(top_logs) > TOP_K:\n",
    "            heapq.heappop(top_logs)\n",
    "\n",
    "        if log.get(Key.LEVEL) == Key.ERROR:\n",
    "            msg = log.get(Key.MESSAGE,'')\n",
    "            err_count = error_counter.get(msg, 0) + 1\n",
    "            error_counter[msg] = err_count\n",
    "            \n",
    "            heapq.heappush(top_errors, (err_count, msg))\n",
    "            if len(top_errors) > TOP_K:\n",
    "                heapq.heappop(top_errors)\n",
    "\n",
    "\n",
    "print('log per level:', log_level_freq)\n",
    "print('\\nerror per service:', error_service_freq)\n",
    "\n",
    "most_occured_logs = sorted(top_logs, reverse=True)\n",
    "most_occured_errors = sorted(top_errors, reverse=True)\n",
    "\n",
    "print(\"\\nmost occured errors: \", most_occured_errors)\n",
    "print(\"\\nmost occured logs: \", most_occured_logs)\n",
    "\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"\\nExecution time: {execution_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9aa038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/output.csv generated\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "def mask_phone(phone):\n",
    "    phone = str(phone).strip()\n",
    "    return 'X' * (len(phone) - 4) + phone[-4:]\n",
    "\n",
    "def hash_email(email):\n",
    "    email = str(email).strip()\n",
    "    return hashlib.sha256(email.encode()).hexdigest()\n",
    "\n",
    "df = pd.read_csv('data/input.csv')\n",
    "\n",
    "\n",
    "df['email'] = df['email'].apply(hash_email)\n",
    "df['phone'] = df['phone'].apply(mask_phone)\n",
    "\n",
    "df.to_csv('data/output.csv', index=False)\n",
    "\n",
    "print(\"data/output.csv generated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2116a35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a@mail.com', 'b@mail.com', 'c@mail.com', 'd@mail.com']\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicate_emails(emails):\n",
    "    seen = set()\n",
    "    unique_emails = []\n",
    "    for email in emails:\n",
    "        if email not in seen:\n",
    "            seen.add(email)\n",
    "            unique_emails.append(email)\n",
    "    return unique_emails\n",
    "\n",
    "\n",
    "\n",
    "emails = [\n",
    "    \"a@mail.com\", \"b@mail.com\", \"a@mail.com\", \"c@mail.com\",\n",
    "    \"b@mail.com\", \"d@mail.com\", \"a@mail.com\"\n",
    "]\n",
    "\n",
    "result = remove_duplicate_emails(emails)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e301054c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
